{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63e6673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Y] n=874 | counts -> 0=608  1=266\n",
      "[INFO] Detected PLINK basenames:\n",
      "  - /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/ADNI1_GWAS_PLINK/ADNI_cluster_01_forward_757LONI\n",
      "  - /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/ADNI_GO2_GWAS_PLINK/ADNI_GO_2_Forward_Bin\n",
      "  - /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/ADNI_GO2_GWAS_PLINK2/ADNI_GO2_GWAS_2nd_orig_BIN\n",
      "  - /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/ADNI3_GWAS_PLINK1/ADNI3_PLINK_Final\n",
      "  - /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/ADNI3_GWAS_PLINK2/ADNI3_PLINK_FINAL_2nd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping files: 100%|██████████| 3/3 [00:01<00:00,  2.55it/s]\n",
      "Mapping files: 100%|██████████| 3/3 [00:00<00:00,  4.30it/s]\n",
      "Mapping files: 100%|██████████| 3/3 [00:00<00:00,  5.98it/s]\n",
      "Mapping files: 100%|██████████| 3/3 [00:00<00:00,  6.09it/s]\n",
      "Mapping files: 100%|██████████| 3/3 [00:00<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] markers.txt written: 74886 SNPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping files: 100%|██████████| 3/3 [00:00<00:00,  9.27it/s]\n",
      "/var/folders/v_/jty2jbj93732jj_tv204kfsr0000gn/T/ipykernel_2949/4128513272.py:181: RuntimeWarning: Mean of empty slice\n",
      "  col_means = np.nanmean(sub, axis=0)\n",
      "Mapping files: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]\n",
      "/var/folders/v_/jty2jbj93732jj_tv204kfsr0000gn/T/ipykernel_2949/4128513272.py:181: RuntimeWarning: Mean of empty slice\n",
      "  col_means = np.nanmean(sub, axis=0)\n",
      "Mapping files: 100%|██████████| 3/3 [00:00<00:00,  7.34it/s]\n",
      "Mapping files: 100%|██████████| 3/3 [00:00<00:00,  5.22it/s]\n",
      "/var/folders/v_/jty2jbj93732jj_tv204kfsr0000gn/T/ipykernel_2949/4128513272.py:181: RuntimeWarning: Mean of empty slice\n",
      "  col_means = np.nanmean(sub, axis=0)\n",
      "Mapping files: 100%|██████████| 3/3 [00:00<00:00,  8.72it/s]\n",
      "/var/folders/v_/jty2jbj93732jj_tv204kfsr0000gn/T/ipykernel_2949/4128513272.py:181: RuntimeWarning: Mean of empty slice\n",
      "  col_means = np.nanmean(sub, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] G_all shape: (2205, 74886) | samples_raw.txt n=2205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Exon tagging: total=74886 | exonic=3262 (4.36%) | non-exonic=71624 (95.64%)\n",
      "[CLEAN] X: removed 14 columns with NaN/Inf; kept 3248.\n",
      "[CLEAN] C: removed 346 columns with NaN/Inf; kept 71278.\n",
      "[ALIGN] X=(815, 3248), C=(815, 71278), Y=(815,), IDs=(815,)\n",
      "[INFO] Using PTID col: PTID | sex src: PTGENDER | gender src: PTGENDER\n",
      "[COV] covariates shape: (815, 2)\n",
      "\n",
      "[DONE] Saved aligned datasets to: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Prepared_data4\n",
      "       - X.npy, C.npy, Y.npy, covariates.npy, ids.txt\n",
      "       - markers.tsv, exonMarkers.txt, nonExonMarkers.txt, is_exonic.npy\n",
      "       - REPORT.txt\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ADNI preprocessing (one-pass, simplified & robust):\n",
    "- Build Y (baseline CN/AD), PTID list\n",
    "- Read multiple PLINK sets, take common SNPs (keeping order of the first set)\n",
    "- Filter invalid loci, assemble genotype matrix (samples x SNPs)\n",
    "- Overlap with Gencode v19 (hg19) to label exonic vs non-exonic\n",
    "- Split to X (exonic), C (non-exonic)\n",
    "- Deduplicate & align to baseline Y by PTID (keep first occurrence)\n",
    "- Build covariates [sex_male, gender_male] (robust to duplicate column names)\n",
    "- Save aligned X, C, Y, covariates, ids to Prepared_data2\n",
    "\n",
    "Requirements:\n",
    "  pip install pandas numpy pandas-plink pyranges scikit-learn\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import gzip, shutil, re, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_plink import read_plink\n",
    "import pyranges as pr\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# ---------- Paths ----------\n",
    "ROOT = Path(\"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data\")\n",
    "OUT  = ROOT / \"Prepared_data4\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DIAG_CSV = ROOT / \"ADNI_Diagnosis.csv\"\n",
    "DEMO_CSV = ROOT / \"ADNI_Demographics.csv\"\n",
    "\n",
    "# Gencode v19 (hg19/GRCh37)\n",
    "GTF_GZ    = ROOT / \"gencode.v19.annotation.gtf.gz\"\n",
    "GTF_PLAIN = ROOT / \"gencode.v19.annotation.gtf\"\n",
    "GTF = GTF_PLAIN if GTF_PLAIN.exists() else GTF_GZ\n",
    "\n",
    "PLINK_DIRS = [\n",
    "    ROOT / \"ADNI1_GWAS_PLINK\",\n",
    "    ROOT / \"ADNI_GO2_GWAS_PLINK\",\n",
    "    ROOT / \"ADNI_GO2_GWAS_PLINK2\",\n",
    "    ROOT / \"ADNI3_GWAS_PLINK1\",\n",
    "    ROOT / \"ADNI3_GWAS_PLINK2\",\n",
    "]\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "_ptid_pat = re.compile(r\"(\\d{3}_S_\\d{4})\")\n",
    "\n",
    "def norm_ptid(s: str) -> str | None:\n",
    "    s = str(s).strip()\n",
    "    m = _ptid_pat.search(s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def detect_bfile(folder: Path) -> Path:\n",
    "    cands = []\n",
    "    for bed in folder.glob(\"*.bed\"):\n",
    "        stem = bed.with_suffix(\"\")\n",
    "        if stem.with_suffix(\".bim\").exists() and stem.with_suffix(\".fam\").exists():\n",
    "            cands.append(stem)\n",
    "    if len(cands) != 1:\n",
    "        raise RuntimeError(f\"[{folder.name}] expected 1 PLINK trio, got {len(cands)}: {cands}\")\n",
    "    return cands[0]\n",
    "\n",
    "def split_plink(a, b, c):\n",
    "    \"\"\"Identify (G, bim, fam) robustly.\"\"\"\n",
    "    objs = [a, b, c]\n",
    "    dfs  = [o for o in objs if hasattr(o, \"columns\")]\n",
    "    arrs = [o for o in objs if not hasattr(o, \"columns\")]\n",
    "    if not (len(dfs) == 2 and len(arrs) == 1):\n",
    "        raise TypeError(f\"Unexpected read_plink returns: {[type(o) for o in objs]}\")\n",
    "    def is_bim(df): return {'chrom','pos','snp'}.issubset(set(df.columns))\n",
    "    def is_fam(df): return 'iid' in df.columns\n",
    "    if is_bim(dfs[0]) and is_fam(dfs[1]): return arrs[0], dfs[0], dfs[1]\n",
    "    if is_bim(dfs[1]) and is_fam(dfs[0]): return arrs[0], dfs[1], dfs[0]\n",
    "    raise ValueError(f\"Cannot detect BIM/FAM. DF1 cols={list(dfs[0].columns)}, DF2 cols={list(dfs[1].columns)}\")\n",
    "\n",
    "def to_chr_label(val) -> str:\n",
    "    s = str(val)\n",
    "    if s in ['23','X','x']: return 'chrX'\n",
    "    if s in ['24','Y','y']: return 'chrY'\n",
    "    if s in ['25','MT','Mt','mt','M','m']: return 'chrM'\n",
    "    if s.lower().startswith('chr'):\n",
    "        return s if s.startswith('chr') else 'chr' + s.split('chr',1)[-1]\n",
    "    return 'chr' + s\n",
    "\n",
    "def ensure_gtf_plain(gtf_path: Path) -> Path:\n",
    "    if gtf_path.suffix == \".gz\":\n",
    "        if not GTF_PLAIN.exists():\n",
    "            print(\"[INFO] Decompressing GTF…\")\n",
    "            with gzip.open(gtf_path, \"rb\") as f_in, open(GTF_PLAIN, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        return GTF_PLAIN\n",
    "    return gtf_path\n",
    "\n",
    "def get_first_series(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "    \"\"\"Return the first column named `col` as a Series, even if duplicates exist.\"\"\"\n",
    "    obj = df[col]\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj.iloc[:, 0]\n",
    "    return obj\n",
    "\n",
    "def make_male_from_column(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "    s = get_first_series(df, col).astype(str).str.upper().str.strip()\n",
    "    male = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "    male[s.isin([\"M\", \"MALE\", \"1\"])] = 1.0\n",
    "    male[s.isin([\"F\", \"FEMALE\", \"0\", \"2\"])] = 0.0\n",
    "    # fuzzy fallbacks\n",
    "    male[s.str.contains(\"MALE\", na=False)] = 1.0\n",
    "    male[s.str.contains(\"FEM\",  na=False)] = 0.0\n",
    "    return male\n",
    "\n",
    "def drop_nan_inf_cols(A: np.ndarray, name: str):\n",
    "    A = np.asarray(A, dtype=np.float64)\n",
    "    bad_nan = np.isnan(A).any(axis=0)\n",
    "    bad_inf = ~np.isfinite(A).all(axis=0)\n",
    "    keep = ~(bad_nan | bad_inf)\n",
    "    removed = int((~keep).sum())\n",
    "    if removed > 0:\n",
    "        print(f\"[CLEAN] {name}: removed {removed} columns with NaN/Inf; kept {keep.sum()}.\")\n",
    "    else:\n",
    "        print(f\"[CLEAN] {name}: no NaN/Inf columns; kept {A.shape[1]}.\")\n",
    "    return A[:, keep], keep\n",
    "\n",
    "# ---------- 1) Build baseline Y (CN=0, AD=1) ----------\n",
    "diag = pd.read_csv(DIAG_CSV)\n",
    "diag[\"VISCODE\"] = diag[\"VISCODE\"].astype(str).str.lower()\n",
    "bl = diag[(diag[\"VISCODE\"]==\"bl\") & (diag[\"DIAGNOSIS\"].isin([1,3]))].copy()\n",
    "bl[\"Y\"] = bl[\"DIAGNOSIS\"].map({1:0, 3:1}).astype(int)\n",
    "bl[\"PTID\"] = bl[\"PTID\"].astype(str).apply(norm_ptid)\n",
    "y_tbl = bl.dropna(subset=[\"PTID\"])[[\"PTID\",\"Y\"]].drop_duplicates(subset=[\"PTID\"]).reset_index(drop=True)\n",
    "print(f\"[Y] n={len(y_tbl)} | counts -> 0={int((y_tbl['Y']==0).sum())}  1={int((y_tbl['Y']==1).sum())}\")\n",
    "\n",
    "# ---------- 2) Read PLINK batches & build common SNP matrix ----------\n",
    "bfiles = [detect_bfile(d) for d in PLINK_DIRS]\n",
    "print(\"[INFO] Detected PLINK basenames:\")\n",
    "for s in bfiles: print(\"  -\", s)\n",
    "\n",
    "common_snps = None\n",
    "bims = []\n",
    "blocks = []\n",
    "all_ids = []\n",
    "\n",
    "for i, stem in enumerate(bfiles):\n",
    "    a, b, c = read_plink(str(stem))\n",
    "    G, bim, fam = split_plink(a, b, c)  # G: (variants x samples) dask/xarray-like\n",
    "\n",
    "    # Filter invalid\n",
    "    bim = bim.copy()\n",
    "    bim['chrom'] = bim['chrom'].astype(str)\n",
    "    bim = bim[(bim['chrom']!='0') & (bim['pos'].astype(int) > 0)]\n",
    "    snps = bim['snp'].astype(str).tolist()\n",
    "\n",
    "    if common_snps is None:\n",
    "        common_snps = snps\n",
    "    else:\n",
    "        s = set(common_snps).intersection(snps)\n",
    "        common_snps = [x for x in common_snps if x in s]  # keep order of the first batch\n",
    "\n",
    "    bims.append(bim)\n",
    "\n",
    "if not common_snps:\n",
    "    raise ValueError(\"No common SNPs across batches. Check build/coordinates.\")\n",
    "\n",
    "pd.Series(common_snps).to_csv(OUT/\"markers.txt\", index=False, header=False)\n",
    "print(f\"[INFO] markers.txt written: {len(common_snps)} SNPs\")\n",
    "\n",
    "# Assemble matrix per batch (samples x common_snps), stack vertically\n",
    "for bim, stem in zip(bims, bfiles):\n",
    "    snp2idx = {s:i for i,s in enumerate(bim['snp'].astype(str).tolist())}\n",
    "    take = [snp2idx[s] for s in common_snps if s in snp2idx]\n",
    "    if len(take) != len(common_snps):\n",
    "        raise RuntimeError(f\"[{stem.name}] missing some common SNPs unexpectedly.\")\n",
    "\n",
    "    a, b, c = read_plink(str(stem))\n",
    "    G, bim2, fam = split_plink(a, b, c)\n",
    "\n",
    "    sub = G[take, :].compute().astype(np.float32).T  # (samples x variants)\n",
    "\n",
    "    # mean-impute NaNs per column\n",
    "    if np.isnan(sub).any():\n",
    "        col_means = np.nanmean(sub, axis=0)\n",
    "        inds = np.where(np.isnan(sub))\n",
    "        sub[inds] = np.take(col_means, inds[1])\n",
    "\n",
    "    ids = fam['iid'].astype(str).apply(norm_ptid).tolist()\n",
    "    all_ids.extend(ids)\n",
    "    blocks.append(sub)\n",
    "\n",
    "G_all = np.vstack(blocks)  # (n_samples_all x n_common_snps)\n",
    "pd.Series(all_ids).to_csv(OUT/\"samples_raw.txt\", index=False, header=False)\n",
    "print(f\"[INFO] G_all shape: {G_all.shape} | samples_raw.txt n={len(all_ids)}\")\n",
    "\n",
    "# ---------- 3) Exon overlap via Gencode v19 ----------\n",
    "gtf_path = ensure_gtf_plain(GTF)\n",
    "gtf = pr.read_gtf(str(gtf_path))\n",
    "feat_col = \"feature\" if \"feature\" in gtf.df.columns else (\"Feature\" if \"Feature\" in gtf.df.columns else None)\n",
    "if feat_col is None:\n",
    "    raise KeyError(f\"GTF lacks 'feature' column. Got {list(gtf.df.columns)}\")\n",
    "exon = gtf[gtf.df[feat_col] == \"exon\"]\n",
    "\n",
    "# Coordinates from FIRST batch (order = common_snps)\n",
    "bim0 = bims[0].set_index(\"snp\").loc[common_snps].reset_index()\n",
    "df_snps = pd.DataFrame({\n",
    "    \"Chromosome\": bim0[\"chrom\"].apply(to_chr_label),\n",
    "    \"Start\": bim0[\"pos\"].astype(int),\n",
    "    \"End\":   bim0[\"pos\"].astype(int) + 1,\n",
    "    \"Name\":  bim0[\"snp\"].astype(str),\n",
    "})\n",
    "gr_snps = pr.PyRanges(df_snps)\n",
    "\n",
    "ovl_df = gr_snps.join(exon).df\n",
    "exon_names = set(ovl_df[\"Name\"].astype(str))\n",
    "is_exonic = df_snps[\"Name\"].astype(str).isin(exon_names).to_numpy()\n",
    "n_all = len(common_snps); n_ex = int(is_exonic.sum()); n_non = n_all - n_ex\n",
    "print(f\"[INFO] Exon tagging: total={n_all} | exonic={n_ex} ({n_ex/n_all:.2%}) | non-exonic={n_non} ({n_non/n_all:.2%})\")\n",
    "\n",
    "# Save marker aides\n",
    "np.save(OUT/\"is_exonic.npy\", is_exonic.astype(bool))\n",
    "df_snps.assign(is_exonic=is_exonic).to_csv(OUT/\"markers.tsv\", sep=\"\\t\", index=False)\n",
    "with open(OUT/\"exonMarkers.txt\",\"w\") as f:\n",
    "    for rs in df_snps.loc[is_exonic, \"Name\"]: f.write(str(rs) + \"\\n\")\n",
    "with open(OUT/\"nonExonMarkers.txt\",\"w\") as f:\n",
    "    for rs in df_snps.loc[~is_exonic, \"Name\"]: f.write(str(rs) + \"\\n\")\n",
    "\n",
    "# ---------- 4) Split to X/C, clean NaN/Inf columns ----------\n",
    "X_full = G_all[:, is_exonic]\n",
    "C_full = G_all[:, ~is_exonic]\n",
    "X_full, maskX = drop_nan_inf_cols(X_full, \"X\")\n",
    "C_full, maskC = drop_nan_inf_cols(C_full, \"C\")\n",
    "\n",
    "# Sync marker lists after cleaning\n",
    "exon_markers_after = df_snps.loc[is_exonic, \"Name\"].astype(str).to_numpy()[maskX]\n",
    "nonexon_markers_after = df_snps.loc[~is_exonic, \"Name\"].astype(str).to_numpy()[maskC]\n",
    "pd.Series(exon_markers_after).to_csv(OUT/\"exonMarkers_after_clean.txt\", index=False, header=False)\n",
    "pd.Series(nonexon_markers_after).to_csv(OUT/\"nonExonMarkers_after_clean.txt\", index=False, header=False)\n",
    "\n",
    "# ---------- 5) Deduplicate PTIDs, align to baseline Y ----------\n",
    "ids_raw = pd.Series(all_ids, name=\"PTID_raw\").to_frame()\n",
    "ids_raw[\"PTID\"] = ids_raw[\"PTID_raw\"].astype(str).apply(norm_ptid)\n",
    "ids_raw = ids_raw.dropna(subset=[\"PTID\"]).reset_index(drop=True)\n",
    "\n",
    "dup_mask = ids_raw.duplicated(subset=[\"PTID\"], keep=\"first\")\n",
    "keep_rows = (~dup_mask).to_numpy()\n",
    "ids_dedup = ids_raw.loc[~dup_mask, [\"PTID\"]].reset_index(drop=True)\n",
    "\n",
    "X_dedup = X_full[keep_rows, :]\n",
    "C_dedup = C_full[keep_rows, :]\n",
    "\n",
    "# left-join on PTID to get Y; keep only rows with Y\n",
    "merged = ids_dedup.merge(y_tbl, on=\"PTID\", how=\"left\")\n",
    "mask_have_y = merged[\"Y\"].notna().to_numpy()\n",
    "\n",
    "X = X_dedup[mask_have_y, :]\n",
    "C = C_dedup[mask_have_y, :]\n",
    "Y = merged.loc[mask_have_y, \"Y\"].to_numpy(dtype=int)\n",
    "IDs = merged.loc[mask_have_y, \"PTID\"].to_numpy()\n",
    "\n",
    "print(f\"[ALIGN] X={X.shape}, C={C.shape}, Y={Y.shape}, IDs={IDs.shape}\")\n",
    "\n",
    "# ---------- 6) Build covariates [sex_male, gender_male] ----------\n",
    "demo = pd.read_csv(DEMO_CSV)\n",
    "\n",
    "# show duplicate columns info (visibility)\n",
    "dup_info = demo.columns.to_series().groupby(demo.columns).size()\n",
    "dups = dup_info[dup_info > 1]\n",
    "if not dups.empty:\n",
    "    print(\"[WARN] Duplicate column names detected in demographics:\\n\", dups)\n",
    "\n",
    "# robustly pick PTID column\n",
    "ptid_col = None\n",
    "for c in [\"PTID\",\"ptid\",\"Subject\",\"subject\",\"PTIDNUM\"]:\n",
    "    if c in demo.columns:\n",
    "        ptid_col = c; break\n",
    "if ptid_col is None:\n",
    "    raise KeyError(\"No PTID-like column in ADNI_Demographics.csv\")\n",
    "\n",
    "# choose two sources (allow duplicates)\n",
    "cands = [\"SEX\",\"Sex\",\"PTGENDER\",\"GENDER\"]\n",
    "sex_src = None; gender_src = None\n",
    "for c in cands:\n",
    "    if c in demo.columns:\n",
    "        if sex_src is None: sex_src = c\n",
    "        elif gender_src is None and c != sex_src: gender_src = c\n",
    "if sex_src is None and gender_src is None:\n",
    "    raise KeyError(\"No sex/gender columns found in demographics.\")\n",
    "\n",
    "if sex_src is None:    sex_src = gender_src\n",
    "if gender_src is None: gender_src = sex_src\n",
    "\n",
    "print(f\"[INFO] Using PTID col: {ptid_col} | sex src: {sex_src} | gender src: {gender_src}\")\n",
    "\n",
    "tmp = demo[[ptid_col, sex_src, gender_src]].copy()\n",
    "tmp[ptid_col] = tmp[ptid_col].astype(str).apply(norm_ptid)\n",
    "tmp = tmp.dropna(subset=[ptid_col]).drop_duplicates(subset=[ptid_col]).reset_index(drop=True)\n",
    "\n",
    "tmp[\"sex_male\"]    = make_male_from_column(tmp, sex_src)\n",
    "tmp[\"gender_male\"] = make_male_from_column(tmp, gender_src)\n",
    "\n",
    "cov = pd.DataFrame({\"PTID\": IDs}).merge(\n",
    "    tmp[[ptid_col, \"sex_male\", \"gender_male\"]].rename(columns={ptid_col:\"PTID\"}),\n",
    "    on=\"PTID\", how=\"left\"\n",
    ")\n",
    "\n",
    "# fill missing by mode (default 0)\n",
    "for col in [\"sex_male\",\"gender_male\"]:\n",
    "    mode = cov[col].dropna().mode()\n",
    "    fill = float(mode.iloc[0]) if not mode.empty else 0.0\n",
    "    cov[col] = cov[col].astype(float).fillna(fill)\n",
    "\n",
    "covariates = cov[[\"sex_male\",\"gender_male\"]].to_numpy(dtype=float)\n",
    "# match old pipeline: L2 normalize columns\n",
    "covariates = normalize(covariates, axis=0)\n",
    "\n",
    "print(f\"[COV] covariates shape: {covariates.shape}\")\n",
    "\n",
    "# ---------- 7) Save aligned outputs to Prepared_data2 ----------\n",
    "np.save(OUT/\"X.npy\", X)\n",
    "np.save(OUT/\"C.npy\", C)\n",
    "np.save(OUT/\"Y.npy\", Y)\n",
    "np.save(OUT/\"covariates.npy\", covariates)\n",
    "pd.Series(IDs).to_csv(OUT/\"ids.txt\", index=False, header=False)\n",
    "\n",
    "# helpful metadata\n",
    "with open(OUT/\"REPORT.txt\",\"w\") as f:\n",
    "    f.write(f\"X shape: {X.shape}\\nC shape: {C.shape}\\nY shape: {Y.shape}\\n\")\n",
    "    f.write(f\"covariates shape: {covariates.shape}\\n\")\n",
    "    f.write(f\"IDs n: {len(IDs)}\\n\")\n",
    "    f.write(f\"exonic kept: {X.shape[1]} | non-exonic kept: {C.shape[1]}\\n\")\n",
    "\n",
    "print(\"\\n[DONE] Saved aligned datasets to:\", OUT)\n",
    "print(\"       - X.npy, C.npy, Y.npy, covariates.npy, ids.txt\")\n",
    "print(\"       - markers.tsv, exonMarkers.txt, nonExonMarkers.txt, is_exonic.npy\")\n",
    "print(\"       - REPORT.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a307fb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] usable genes: 71272 / 71278\n",
      "[SAVE] C_silhouette_top3000.npy shape=(815, 3000)\n",
      "[SAVE] C_silhouette_top3000_markers.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "PREP = Path(\"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Prepared_data4\")\n",
    "C = np.load(PREP / \"C.npy\")\n",
    "\n",
    "# 标准化 & 近零方差过滤\n",
    "Z = StandardScaler(with_mean=True, with_std=True).fit_transform(C)\n",
    "std = Z.std(axis=0)\n",
    "keep = std > 1e-8\n",
    "Z = Z[:, keep]\n",
    "orig_cols = np.flatnonzero(keep)\n",
    "print(f\"[INFO] usable genes: {Z.shape[1]} / {C.shape[1]}\")\n",
    "\n",
    "# k=2 的 silhouette；最小簇≥10%\n",
    "def sil_one(x):\n",
    "    x = x.reshape(-1,1)\n",
    "    lab = KMeans(n_clusters=2, n_init=10, random_state=0).fit_predict(x)\n",
    "    if min(np.bincount(lab)) / len(lab) < 0.10:\n",
    "        return np.nan\n",
    "    return silhouette_score(x, lab, metric=\"euclidean\")\n",
    "\n",
    "scores = np.array([sil_one(Z[:, j]) for j in range(Z.shape[1])])\n",
    "\n",
    "# 分布图\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(scores[np.isfinite(scores)], bins=60, edgecolor=\"white\")\n",
    "plt.xlabel(\"Silhouette score (k=2)\"); plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of per-gene Silhouette scores\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PREP / \"C_silhouette_distribution_k2.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# Top-3000\n",
    "valid = np.where(np.isfinite(scores))[0]\n",
    "topk = min(3000, valid.size)\n",
    "top_idx_local = valid[np.argsort(scores[valid])[::-1][:topk]]\n",
    "top_idx_orig = orig_cols[top_idx_local]\n",
    "\n",
    "C_top = C[:, top_idx_orig]\n",
    "np.save(PREP / \"C_silhouette_top3000.npy\", C_top)\n",
    "np.save(PREP / \"C_silhouette_top3000_colidx.npy\", top_idx_orig)\n",
    "print(f\"[SAVE] C_silhouette_top3000.npy shape={C_top.shape}\")\n",
    "\n",
    "# 可选：输出对应的标记名\n",
    "markers_path = PREP / \"nonExonMarkers_after_clean.txt\"\n",
    "if markers_path.exists():\n",
    "    mk = pd.read_csv(markers_path, header=None)[0].astype(str)\n",
    "    if len(mk) == C.shape[1]:\n",
    "        mk_sel = mk.iloc[top_idx_orig].reset_index(drop=True)\n",
    "        mk_sel.to_csv(PREP / \"C_silhouette_top3000_markers.txt\", index=False, header=False)\n",
    "        print(\"[SAVE] C_silhouette_top3000_markers.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
