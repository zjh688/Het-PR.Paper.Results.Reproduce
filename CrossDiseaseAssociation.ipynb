{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c66fbe",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "__author__ = 'Jiahui Zhang'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290616a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 每个文件中不同 SNP 的数量 =====\n",
      "ADHD: 2317 SNPs\n",
      "ALZ: 259 SNPs\n",
      "ASD: 1260 SNPs\n",
      "EPILEPSY: 330 SNPs\n",
      "MEMORY: 4034 SNPs\n",
      "\n",
      "===== ALZ 与其他疾病 SNP 的重合情况 =====\n",
      "\n",
      "ALZ vs ADHD:\n",
      "  重合 SNP 数: 0\n",
      "  占 ALZ 的比例: 0.00%\n",
      "  占 ADHD 的比例: 0.00%\n",
      "\n",
      "ALZ vs ASD:\n",
      "  重合 SNP 数: 0\n",
      "  占 ALZ 的比例: 0.00%\n",
      "  占 ASD 的比例: 0.00%\n",
      "\n",
      "ALZ vs EPILEPSY:\n",
      "  重合 SNP 数: 0\n",
      "  占 ALZ 的比例: 0.00%\n",
      "  占 EPILEPSY 的比例: 0.00%\n",
      "\n",
      "ALZ vs MEMORY:\n",
      "  重合 SNP 数: 5\n",
      "  占 ALZ 的比例: 1.93%\n",
      "  占 MEMORY 的比例: 0.12%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. 路径和文件名设置 ===\n",
    "base_dir = \"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP\"\n",
    "\n",
    "files = {\n",
    "    \"ADHD\": \"ADHD_SNP.tsv\",\n",
    "    \"ALZ\": \"ALZ_SNP.tsv\",\n",
    "    \"ASD\": \"ASD_SNP.tsv\",\n",
    "    \"EPILEPSY\": \"EPILE_SNP.tsv\",\n",
    "    \"MEMORY\": \"MP_SNP.tsv\",\n",
    "}\n",
    "\n",
    "# === 2. 读取文件，提取 SNP 集合 ===\n",
    "snp_sets = {}\n",
    "counts = {}\n",
    "dfs = {}  # 保存 DataFrame，后面写 cleaned 文件要用\n",
    "\n",
    "for name, fname in files.items():\n",
    "    path = os.path.join(base_dir, fname)\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    dfs[name] = df\n",
    "\n",
    "    # 有些文件里 SNPS 可能不是字符串，统一转一下\n",
    "    df[\"SNPS\"] = df[\"SNPS\"].astype(str)\n",
    "\n",
    "    snp_set = set(df[\"SNPS\"])\n",
    "    snp_sets[name] = snp_set\n",
    "    counts[name] = len(snp_set)\n",
    "\n",
    "# === 3. 打印每个文件的 SNP 总数 ===\n",
    "print(\"===== 每个文件中不同 SNP 的数量 =====\")\n",
    "for name in files.keys():\n",
    "    print(f\"{name}: {counts[name]} SNPs\")\n",
    "\n",
    "# === 4. 计算 ALZ 和其他四个文件的 overlap 情况 ===\n",
    "alz_snps = snp_sets[\"ALZ\"]\n",
    "\n",
    "print(\"\\n===== ALZ 与其他疾病 SNP 的重合情况 =====\")\n",
    "for name in files.keys():\n",
    "    if name == \"ALZ\":\n",
    "        continue\n",
    "\n",
    "    other_snps = snp_sets[name]\n",
    "    overlap = alz_snps.intersection(other_snps)\n",
    "    overlap_count = len(overlap)\n",
    "\n",
    "    # 百分比（*100 后保留两位小数）\n",
    "    pct_in_alz = overlap_count / len(alz_snps) * 100 if len(alz_snps) > 0 else 0.0\n",
    "    pct_in_other = overlap_count / len(other_snps) * 100 if len(other_snps) > 0 else 0.0\n",
    "\n",
    "    print(f\"\\nALZ vs {name}:\")\n",
    "    print(f\"  重合 SNP 数: {overlap_count}\")\n",
    "    print(f\"  占 ALZ 的比例: {pct_in_alz:.2f}%\")\n",
    "    print(f\"  占 {name} 的比例: {pct_in_other:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500bd142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ADHD: 已保存 cleaned 文件 -> ADHD_SNP_cleaned.tsv\n",
      "  原始 SNP 数: 2317\n",
      "  删除重合 SNP 数: 0\n",
      "  cleaned 剩余 SNP 数: 2317\n",
      "\n",
      "ASD: 已保存 cleaned 文件 -> ASD_SNP_cleaned.tsv\n",
      "  原始 SNP 数: 1260\n",
      "  删除重合 SNP 数: 0\n",
      "  cleaned 剩余 SNP 数: 1260\n",
      "\n",
      "EPILEPSY: 已保存 cleaned 文件 -> EPILE_SNP_cleaned.tsv\n",
      "  原始 SNP 数: 330\n",
      "  删除重合 SNP 数: 0\n",
      "  cleaned 剩余 SNP 数: 330\n",
      "\n",
      "MEMORY: 已保存 cleaned 文件 -> MP_SNP_cleaned.tsv\n",
      "  原始 SNP 数: 4034\n",
      "  删除重合 SNP 数: 5\n",
      "  cleaned 剩余 SNP 数: 4029\n"
     ]
    }
   ],
   "source": [
    "# === 5. 生成去掉与 ALZ 重合 SNP 的 cleaned 文件 ===\n",
    "for name in files.keys():\n",
    "    if name == \"ALZ\":\n",
    "        continue  # ALZ 自己不改\n",
    "\n",
    "    df = dfs[name].copy()\n",
    "    df[\"SNPS\"] = df[\"SNPS\"].astype(str)\n",
    "\n",
    "    # 找到与 ALZ 重合的 SNP\n",
    "    overlap_snps = set(df[\"SNPS\"]).intersection(alz_snps)\n",
    "\n",
    "    # 保留非重合 SNP 行\n",
    "    cleaned_df = df[~df[\"SNPS\"].isin(overlap_snps)]\n",
    "\n",
    "    # 输出文件名：原来加 _cleaned 后缀\n",
    "    original_fname = files[name]\n",
    "    root, ext = os.path.splitext(original_fname)\n",
    "    cleaned_fname = root + \"_cleaned\" + ext\n",
    "    cleaned_path = os.path.join(base_dir, cleaned_fname)\n",
    "\n",
    "    cleaned_df.to_csv(cleaned_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    print(f\"\\n{name}: 已保存 cleaned 文件 -> {cleaned_fname}\")\n",
    "    print(f\"  原始 SNP 数: {counts[name]}\")\n",
    "    print(f\"  删除重合 SNP 数: {len(overlap_snps)}\")\n",
    "    print(f\"  cleaned 剩余 SNP 数: {len(set(cleaned_df['SNPS']))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957ef483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 中 SNP 总数（nonexonMarkers_after_clean）：71278\n",
      "\n",
      "===== C 的 SNP 与四个 cleaned GWAS 文件的重合情况 =====\n",
      "\n",
      "ADHD:\n",
      "  ADHD_SNP_cleaned.tsv 中 SNP 总数: 2317\n",
      "  与 C 重合 SNP 数: 56\n",
      "  占 C 中 SNP 比例: 0.0786%\n",
      "  占 ADHD 文件 SNP 比例: 2.4169%\n",
      "\n",
      "ASD:\n",
      "  ASD_SNP_cleaned.tsv 中 SNP 总数: 1260\n",
      "  与 C 重合 SNP 数: 49\n",
      "  占 C 中 SNP 比例: 0.0687%\n",
      "  占 ASD 文件 SNP 比例: 3.8889%\n",
      "\n",
      "EPILE:\n",
      "  EPILE_SNP_cleaned.tsv 中 SNP 总数: 330\n",
      "  与 C 重合 SNP 数: 13\n",
      "  占 C 中 SNP 比例: 0.0182%\n",
      "  占 EPILE 文件 SNP 比例: 3.9394%\n",
      "\n",
      "MEMORY:\n",
      "  MP_SNP_cleaned.tsv 中 SNP 总数: 4029\n",
      "  与 C 重合 SNP 数: 39\n",
      "  占 C 中 SNP 比例: 0.0547%\n",
      "  占 MEMORY 文件 SNP 比例: 0.9680%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ===== 路径设置 =====\n",
    "base_root = \"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data\"\n",
    "\n",
    "prepared_dir = os.path.join(base_root, \"Prepared_data4\")\n",
    "related_snp_dir = os.path.join(base_root, \"Related_SNP\")\n",
    "\n",
    "# C 对应的 SNP 名单（non-exonic SNP after cleaning）\n",
    "nonexon_markers_path = os.path.join(prepared_dir, \"nonexonMarkers_after_clean.txt\")\n",
    "\n",
    "# 四个 cleaned 文件\n",
    "cleaned_files = {\n",
    "    \"ADHD\":  \"ADHD_SNP_cleaned.tsv\",\n",
    "    \"ASD\":   \"ASD_SNP_cleaned.tsv\",\n",
    "    \"EPILE\": \"EPILE_SNP_cleaned.tsv\",\n",
    "    \"MEMORY\": \"MP_SNP_cleaned.tsv\",\n",
    "}\n",
    "\n",
    "# ===== 1. 读 C 对应的 SNP 列表 =====\n",
    "c_snps = pd.read_csv(nonexon_markers_path, header=None)[0].astype(str)\n",
    "c_snp_set = set(c_snps)\n",
    "print(f\"C 中 SNP 总数（nonexonMarkers_after_clean）：{len(c_snp_set)}\")\n",
    "\n",
    "# ===== 2. 依次和四个 cleaned 文件求交集 =====\n",
    "print(\"\\n===== C 的 SNP 与四个 cleaned GWAS 文件的重合情况 =====\")\n",
    "for name, fname in cleaned_files.items():\n",
    "    path = os.path.join(related_snp_dir, fname)\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "\n",
    "    # 确保 SNPS 列存在且转成字符串\n",
    "    if \"SNPS\" not in df.columns:\n",
    "        raise KeyError(f\"{fname} 中没有 'SNPS' 这一列，请检查列名。\")\n",
    "\n",
    "    snps = df[\"SNPS\"].astype(str)\n",
    "    snp_set = set(snps)\n",
    "\n",
    "    overlap = c_snp_set.intersection(snp_set)\n",
    "    overlap_count = len(overlap)\n",
    "\n",
    "    # 一些比例统计（可选）\n",
    "    pct_in_c = overlap_count / len(c_snp_set) * 100 if len(c_snp_set) > 0 else 0.0\n",
    "    pct_in_file = overlap_count / len(snp_set) * 100 if len(snp_set) > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  {fname} 中 SNP 总数: {len(snp_set)}\")\n",
    "    print(f\"  与 C 重合 SNP 数: {overlap_count}\")\n",
    "    print(f\"  占 C 中 SNP 比例: {pct_in_c:.4f}%\")\n",
    "    print(f\"  占 {name} 文件 SNP 比例: {pct_in_file:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72cf8d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading C, ids, and SNP markers ...\n",
      "[INFO] Loading AD groups from figure3_groups.csv ...\n",
      "[INFO] AD 病人数: 251, C 中 SNP 数: 71278\n",
      "\n",
      "[INFO] Processing disease: ADHD\n",
      "[INFO] ADHD: cleaned SNP 数=2317, 与 C 重合 SNP 数=56\n",
      "[INFO] ADHD: 结果已保存到 /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/ADHD_C_overlap_ttest_results.tsv\n",
      "\n",
      "[INFO] Processing disease: ASD\n",
      "[INFO] ASD: cleaned SNP 数=1260, 与 C 重合 SNP 数=49\n",
      "[INFO] ASD: 结果已保存到 /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/ASD_C_overlap_ttest_results.tsv\n",
      "\n",
      "[INFO] Processing disease: EPILE\n",
      "[INFO] EPILE: cleaned SNP 数=330, 与 C 重合 SNP 数=13\n",
      "[INFO] EPILE: 结果已保存到 /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/EPILE_C_overlap_ttest_results.tsv\n",
      "\n",
      "[INFO] Processing disease: MEMORY\n",
      "[INFO] MEMORY: cleaned SNP 数=4029, 与 C 重合 SNP 数=39\n",
      "[INFO] MEMORY: 结果已保存到 /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/MEMORY_C_overlap_ttest_results.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangjiahui/anaconda3/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:586: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/Users/zhangjiahui/anaconda3/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:586: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/Users/zhangjiahui/anaconda3/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:586: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/Users/zhangjiahui/anaconda3/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:586: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "使用 Prepared_data4 中的 C.npy + ids.txt + figure3_groups.csv，\n",
    "对四个 cleaned 疾病 SNP 集合中与 C 重合的 SNP 做两组 t 检验（AD 组1 vs 组2）。\n",
    "\n",
    "结果输出到：\n",
    "  /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# ------------ 路径设置 ------------\n",
    "ROOT = Path(\"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data\")\n",
    "PREP = ROOT / \"Prepared_data4\"\n",
    "SNP_DIR = ROOT / \"Related_SNP\"\n",
    "\n",
    "C_PATH = PREP / \"C.npy\"\n",
    "IDS_PATH = PREP / \"ids.txt\"\n",
    "NONEXON_MARKERS_PATH = PREP / \"nonexonMarkers_after_clean.txt\"\n",
    "GROUPS_PATH = PREP / \"figure3_groups.csv\"   # Fig3 脚本生成的 AD-only 分组\n",
    "\n",
    "# 四个 cleaned GWAS SNP 文件\n",
    "CLEANED_FILES = {\n",
    "    \"ADHD\":   SNP_DIR / \"ADHD_SNP_cleaned.tsv\",\n",
    "    \"ASD\":    SNP_DIR / \"ASD_SNP_cleaned.tsv\",\n",
    "    \"EPILE\":  SNP_DIR / \"EPILE_SNP_cleaned.tsv\",\n",
    "    \"MEMORY\": SNP_DIR / \"MP_SNP_cleaned.tsv\",\n",
    "}\n",
    "\n",
    "# ------------ 1. 读取 C, ids, SNP 列顺序 ------------\n",
    "print(\"[INFO] Loading C, ids, and SNP markers ...\")\n",
    "C = np.load(C_PATH)                           # shape: (n_subjects, n_C_snps)\n",
    "ids = np.loadtxt(IDS_PATH, dtype=str)        # length: n_subjects\n",
    "assert C.shape[0] == len(ids), \"C 的行数与 ids 数量不一致。\"\n",
    "\n",
    "# C 的列对应的 SNP 名称（non-exonic）\n",
    "nonexon_snps = pd.read_csv(NONEXON_MARKERS_PATH, header=None)[0].astype(str).to_numpy()\n",
    "assert C.shape[1] == len(nonexon_snps), \"C 的列数与 nonexonMarkers_after_clean.txt 长度不一致。\"\n",
    "\n",
    "snp_to_col = {snp: idx for idx, snp in enumerate(nonexon_snps)}\n",
    "\n",
    "# ------------ 2. 读取 AD 分组（figure3_groups.csv） ------------\n",
    "print(\"[INFO] Loading AD groups from figure3_groups.csv ...\")\n",
    "grp_df = pd.read_csv(GROUPS_PATH)\n",
    "\n",
    "# figure3_groups.csv 中列名是 \"id\" 和 \"cluster\"\n",
    "if \"id\" not in grp_df.columns or \"cluster\" not in grp_df.columns:\n",
    "    raise ValueError(\"figure3_groups.csv 应包含 'id' 和 'cluster' 两列。\")\n",
    "\n",
    "grp_df = grp_df[[\"id\", \"cluster\"]].copy()\n",
    "grp_df[\"id\"] = grp_df[\"id\"].astype(str)\n",
    "\n",
    "# 把 cluster 映射到 {1,2}，保证和 Fig4 的脚本一致\n",
    "uniq_clusters = sorted(pd.unique(grp_df[\"cluster\"].astype(int)))\n",
    "if len(uniq_clusters) < 2:\n",
    "    raise ValueError(\"分组文件中只有一个簇，无法做两组比较。\")\n",
    "cluster_map = {uniq_clusters[0]: 1, uniq_clusters[1]: 2}\n",
    "grp_df[\"group\"] = grp_df[\"cluster\"].astype(int).map(cluster_map)\n",
    "\n",
    "# 只保留 AD 中在 ids 里的那些\n",
    "id_to_idx = {sid: i for i, sid in enumerate(ids)}\n",
    "grp_df = grp_df[grp_df[\"id\"].isin(id_to_idx.keys())].copy()\n",
    "\n",
    "ad_indices = np.array([id_to_idx[sid] for sid in grp_df[\"id\"]], dtype=int)\n",
    "groups = grp_df[\"group\"].to_numpy(int)\n",
    "assert ad_indices.shape[0] == groups.shape[0], \"AD index 与 group 长度不一致。\"\n",
    "\n",
    "# 在 AD 病人子集上取 C\n",
    "C_ad = C[ad_indices, :]     # shape: (n_AD, n_C_snps)\n",
    "\n",
    "print(f\"[INFO] AD 病人数: {C_ad.shape[0]}, C 中 SNP 数: {C_ad.shape[1]}\")\n",
    "\n",
    "# ------------ 3. 对四个 cleaned GWAS 集中的 overlap SNP 做 t-test ------------\n",
    "for disease, path in CLEANED_FILES.items():\n",
    "    print(f\"\\n[INFO] Processing disease: {disease}\")\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "\n",
    "    if \"SNPS\" not in df.columns:\n",
    "        raise KeyError(f\"{path.name} 中缺少 'SNPS' 列。\")\n",
    "\n",
    "    snps_d = df[\"SNPS\"].astype(str)\n",
    "    snp_set_d = set(snps_d)\n",
    "\n",
    "    # 与 C 的 overlap\n",
    "    overlap_snps = sorted(snp_set_d.intersection(snp_to_col.keys()))\n",
    "    print(f\"[INFO] {disease}: cleaned SNP 数={len(snp_set_d)}, 与 C 重合 SNP 数={len(overlap_snps)}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for snp in overlap_snps:\n",
    "        col_idx = snp_to_col.get(snp, None)\n",
    "        if col_idx is None:\n",
    "            continue  # 理论上不会发生\n",
    "\n",
    "        x = C_ad[:, col_idx].astype(float)\n",
    "\n",
    "        g1 = x[groups == 1]\n",
    "        g2 = x[groups == 2]\n",
    "\n",
    "        # 安全检查：两组至少要有两个样本\n",
    "        if len(g1) < 2 or len(g2) < 2:\n",
    "            t_stat, p_val = np.nan, np.nan\n",
    "        else:\n",
    "            t_stat, p_val = ttest_ind(g1, g2, equal_var=False, nan_policy=\"omit\")\n",
    "\n",
    "        results.append({\n",
    "            \"SNP\": snp,\n",
    "            \"C_col_index\": col_idx,\n",
    "            \"n_group1\": len(g1),\n",
    "            \"n_group2\": len(g2),\n",
    "            \"mean_group1\": float(np.mean(g1)) if len(g1) > 0 else np.nan,\n",
    "            \"mean_group2\": float(np.mean(g2)) if len(g2) > 0 else np.nan,\n",
    "            \"t_stat\": float(t_stat) if t_stat is not None else np.nan,\n",
    "            \"p_value\": float(p_val) if p_val is not None else np.nan,\n",
    "        })\n",
    "\n",
    "    if not results:\n",
    "        print(f\"[WARN] {disease}: 没有与 C 重合的 SNP，跳过保存。\")\n",
    "        continue\n",
    "\n",
    "    res_df = pd.DataFrame(results)\n",
    "    # 按 p 值排序一下，方便你后面筛选\n",
    "    res_df = res_df.sort_values(\"p_value\", na_position=\"last\")\n",
    "\n",
    "    out_path = SNP_DIR / f\"{disease}_C_overlap_ttest_results.tsv\"\n",
    "    res_df.to_csv(out_path, sep=\"\\t\", index=False)\n",
    "    print(f\"[INFO] {disease}: 结果已保存到 {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a018ee2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 读取 markers.tsv ...\n",
      "[INFO] 读取 GTF ...\n",
      "\n",
      "[INFO] 处理疾病: ADHD\n",
      "[INFO] ADHD: p < 0.1 的 SNP 数量 = 48\n",
      "[INFO] ADHD: 有 22 个 SNP 未 overlap 到 gene，使用 nearest 填充。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n",
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n",
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] ADHD: 已输出 /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/ADHD_C_overlap_p0.1_annotated.csv\n",
      "\n",
      "[INFO] 处理疾病: ASD\n",
      "[INFO] ASD: p < 0.1 的 SNP 数量 = 42\n",
      "[INFO] ASD: 有 19 个 SNP 未 overlap 到 gene，使用 nearest 填充。\n",
      "[DONE] ASD: 已输出 /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/ASD_C_overlap_p0.1_annotated.csv\n",
      "\n",
      "[INFO] 处理疾病: EPILE\n",
      "[INFO] EPILE: p < 0.1 的 SNP 数量 = 13\n",
      "[INFO] EPILE: 有 3 个 SNP 未 overlap 到 gene，使用 nearest 填充。\n",
      "[DONE] EPILE: 已输出 /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/EPILE_C_overlap_p0.1_annotated.csv\n",
      "\n",
      "[INFO] 处理疾病: MEMORY\n",
      "[INFO] MEMORY: p < 0.1 的 SNP 数量 = 32\n",
      "[INFO] MEMORY: 有 18 个 SNP 未 overlap 到 gene，使用 nearest 填充。\n",
      "[DONE] MEMORY: 已输出 /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/MEMORY_C_overlap_p0.1_annotated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n",
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n",
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n",
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n",
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "从四个 *_C_overlap_ttest_results.tsv 中筛选 p_value < 0.1 的 SNP，\n",
    "并用 Prepared_data4/markers.tsv + gencode.v19.annotation.gtf 做基因注释。\n",
    "\n",
    "输出：在 Related_SNP 目录下生成 4 个 CSV 文件，比如：\n",
    "  ADHD_C_overlap_p0.1_annotated.csv\n",
    "  ASD_C_overlap_p0.1_annotated.csv\n",
    "  EPILE_C_overlap_p0.1_annotated.csv\n",
    "  MEMORY_C_overlap_p0.1_annotated.csv\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyranges as pr\n",
    "\n",
    "# ===== 路径 =====\n",
    "BASE = Path(\"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data\")\n",
    "PREP = BASE / \"Prepared_data4\"\n",
    "SNP_DIR = BASE / \"Related_SNP\"\n",
    "\n",
    "GTF_PATH = BASE / \"gencode.v19.annotation.gtf\"\n",
    "MARKERS_TSV = PREP / \"markers.tsv\"\n",
    "\n",
    "# 四个 t-test 结果文件\n",
    "TTEST_FILES = {\n",
    "    \"ADHD\":   SNP_DIR / \"ADHD_C_overlap_ttest_results.tsv\",\n",
    "    \"ASD\":    SNP_DIR / \"ASD_C_overlap_ttest_results.tsv\",\n",
    "    \"EPILE\":  SNP_DIR / \"EPILE_C_overlap_ttest_results.tsv\",\n",
    "    \"MEMORY\": SNP_DIR / \"MEMORY_C_overlap_ttest_results.tsv\",\n",
    "}\n",
    "\n",
    "# ===== 公用小工具 =====\n",
    "def ensure_chr_prefix(s):\n",
    "    s = str(s)\n",
    "    return s if s.lower().startswith(\"chr\") else \"chr\" + s\n",
    "\n",
    "def pick_col(cols, cands):\n",
    "    \"\"\"在 cols 中找候选列名之一，兼容大小写。\"\"\"\n",
    "    for c in cands:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for c in cands:\n",
    "        if c.lower() in low:\n",
    "            return low[c.lower()]\n",
    "    return None\n",
    "\n",
    "def agg_join(series):\n",
    "    vals = series.dropna().astype(str).unique().tolist()\n",
    "    return \";\".join(vals) if vals else \"\"\n",
    "\n",
    "# ===== 预读 markers.tsv 和 GTF =====\n",
    "print(\"[INFO] 读取 markers.tsv ...\")\n",
    "m = pd.read_csv(MARKERS_TSV, sep=\"\\t\")\n",
    "need_cols = [\"Name\", \"Chromosome\", \"Start\", \"End\"]\n",
    "missing = [c for c in need_cols if c not in m.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"{MARKERS_TSV} 缺少列: {missing}\")\n",
    "m = m[need_cols].copy()\n",
    "m[\"Name\"] = m[\"Name\"].astype(str)\n",
    "\n",
    "print(\"[INFO] 读取 GTF ...\")\n",
    "gtf = pr.read_gtf(str(GTF_PATH))\n",
    "feat_col = \"feature\" if \"feature\" in gtf.df.columns else (\"Feature\" if \"Feature\" in gtf.df.columns else None)\n",
    "if feat_col is None:\n",
    "    raise KeyError(f\"GTF 缺少 feature 列。实际列: {list(gtf.df.columns)}\")\n",
    "genes = gtf[gtf.df[feat_col] == \"gene\"]\n",
    "\n",
    "# 方便后面 nearest 用\n",
    "gr_genes = genes\n",
    "\n",
    "# ===== 主循环：对四个疾病分别处理 =====\n",
    "for disease, ttest_path in TTEST_FILES.items():\n",
    "    print(f\"\\n[INFO] 处理疾病: {disease}\")\n",
    "    if not ttest_path.exists():\n",
    "        print(f\"[WARN] {ttest_path} 不存在，跳过。\")\n",
    "        continue\n",
    "\n",
    "    res = pd.read_csv(ttest_path, sep=\"\\t\")\n",
    "    # 要有 \"SNP\" 和 \"p_value\"\n",
    "    if \"SNP\" not in res.columns or \"p_value\" not in res.columns:\n",
    "        raise ValueError(f\"{ttest_path.name} 中必须包含 'SNP' 和 'p_value' 列。\")\n",
    "\n",
    "    # 筛选 p_value < 0.1\n",
    "    sig = res[res[\"p_value\"] < 0.1].copy()\n",
    "    sig[\"SNP\"] = sig[\"SNP\"].astype(str)\n",
    "    if sig.empty:\n",
    "        print(f\"[INFO] {disease}: p_value < 0.1 的 SNP 为空，将仍输出一个只有表头的文件。\")\n",
    "        # 直接生成空文件，附上列名\n",
    "        out_empty = SNP_DIR / f\"{disease}_C_overlap_p0.1_annotated.csv\"\n",
    "        sig.to_csv(out_empty, index=False)\n",
    "        continue\n",
    "\n",
    "    sig_snps = sig[\"SNP\"].tolist()\n",
    "    print(f\"[INFO] {disease}: p < 0.1 的 SNP 数量 = {len(sig_snps)}\")\n",
    "\n",
    "    # ===== 从 markers.tsv 中取这些 SNP 的坐标 =====\n",
    "    m_sub = m[m[\"Name\"].isin(sig_snps)].copy()\n",
    "    if m_sub.empty:\n",
    "        print(f\"[WARN] {disease}: 在 markers.tsv 中找不到这些 SNP 的坐标，将仅输出 t-test 结果。\")\n",
    "        out_path = SNP_DIR / f\"{disease}_C_overlap_p0.1_annotated.csv\"\n",
    "        sig.to_csv(out_path, index=False)\n",
    "        continue\n",
    "\n",
    "    # 按 sig_snps 顺序对齐\n",
    "    order_map = {s: i for i, s in enumerate(sig_snps)}\n",
    "    m_sub[\"__order__\"] = m_sub[\"Name\"].map(order_map)\n",
    "    m_sub = m_sub.sort_values(\"__order__\").drop(columns=\"__order__\")\n",
    "\n",
    "    # 构造 PyRanges 输入\n",
    "    snps_df = pd.DataFrame({\n",
    "        \"Chromosome\": m_sub[\"Chromosome\"].astype(str).map(ensure_chr_prefix),\n",
    "        \"Start\": m_sub[\"Start\"].astype(int),\n",
    "        \"End\": m_sub[\"End\"].astype(int),\n",
    "        \"SNP\": m_sub[\"Name\"].astype(str),\n",
    "    })\n",
    "    gr_snps = pr.PyRanges(snps_df)\n",
    "\n",
    "    # ===== overlap: SNP 落在 gene 区间上 =====\n",
    "    ovl = gr_snps.join(genes).df\n",
    "\n",
    "    # 找基因相关列名\n",
    "    gene_id_col   = pick_col(ovl.columns, [\"gene_id\", \"Gene_id\", \"geneID\"])\n",
    "    gene_name_col = pick_col(ovl.columns, [\"gene_name\", \"Gene_name\", \"gene\", \"gene_symbol\"])\n",
    "    gene_type_col = pick_col(ovl.columns, [\"gene_type\", \"Gene_type\", \"gene_biotype\", \"Gene_biotype\", \"biotype\"])\n",
    "\n",
    "    if len(ovl) > 0:\n",
    "        ovl_grouped = (ovl\n",
    "            .groupby(\"SNP\", as_index=False)\n",
    "            .agg({\n",
    "                \"Chromosome\": \"first\",\n",
    "                \"Start\":      \"first\",\n",
    "                gene_id_col:   agg_join if gene_id_col   else (lambda s: \"\"),\n",
    "                gene_name_col: agg_join if gene_name_col else (lambda s: \"\"),\n",
    "                gene_type_col: agg_join if gene_type_col else (lambda s: \"\"),\n",
    "            })\n",
    "        )\n",
    "    else:\n",
    "        # 没有任何 overlap 的情况\n",
    "        ovl_grouped = pd.DataFrame(columns=[\"SNP\",\"Chromosome\",\"Start\"])\n",
    "\n",
    "    # 标准列名\n",
    "    col_map = {}\n",
    "    if gene_id_col:   col_map[gene_id_col]   = \"Gene ID\"\n",
    "    if gene_name_col: col_map[gene_name_col] = \"Gene Name\"\n",
    "    if gene_type_col: col_map[gene_type_col] = \"Gene Type\"\n",
    "    ovl_grouped = ovl_grouped.rename(columns=col_map)\n",
    "    ovl_grouped = ovl_grouped.rename(columns={\"Start\": \"Location\"})\n",
    "\n",
    "    # 基础表：所有 sig SNP + 坐标（即使没 overlap 到 gene）\n",
    "    base = snps_df.rename(columns={\"Start\": \"Location\"})[[\"SNP\", \"Chromosome\", \"Location\"]]\n",
    "    annot = base.merge(\n",
    "        ovl_grouped[[\"SNP\",\"Gene ID\",\"Gene Name\",\"Gene Type\"]] if len(ovl_grouped) else base.assign(**{\"Gene ID\":np.nan,\"Gene Name\":np.nan,\"Gene Type\":np.nan}),\n",
    "        on=\"SNP\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # ===== 对没有任何 gene 注释的 SNP，用最近基因补充 =====\n",
    "    missing_mask = annot[\"Gene ID\"].isna() & annot[\"Gene Name\"].isna() & annot[\"Gene Type\"].isna()\n",
    "    if missing_mask.any():\n",
    "        print(f\"[INFO] {disease}: 有 {missing_mask.sum()} 个 SNP 未 overlap 到 gene，使用 nearest 填充。\")\n",
    "        # PyRanges 支持布尔掩码，需要对应行\n",
    "        gr_snps_missing = pr.PyRanges(snps_df[missing_mask.values])\n",
    "        nearest_df = gr_snps_missing.nearest(gr_genes).df\n",
    "\n",
    "        gid = pick_col(nearest_df.columns, [\"gene_id\", \"Gene_id\", \"geneID\"])\n",
    "        gna = pick_col(nearest_df.columns, [\"gene_name\", \"Gene_name\", \"gene\", \"gene_symbol\"])\n",
    "        gty = pick_col(nearest_df.columns, [\"gene_type\", \"Gene_type\", \"gene_biotype\", \"Gene_biotype\", \"biotype\"])\n",
    "\n",
    "        nearest_slim = pd.DataFrame({\n",
    "            \"SNP\": nearest_df[\"SNP\"].astype(str),\n",
    "            \"Gene ID\": nearest_df[gid].astype(str) if gid else \"\",\n",
    "            \"Gene Name\": nearest_df[gna].astype(str) if gna else \"\",\n",
    "            \"Gene Type\": nearest_df[gty].astype(str) if gty else \"\",\n",
    "        })\n",
    "\n",
    "        annot = annot.merge(nearest_slim, on=\"SNP\", how=\"left\", suffixes=(\"\", \"_nearest\"))\n",
    "        for k in [\"Gene ID\",\"Gene Name\",\"Gene Type\"]:\n",
    "            fill_mask = annot[k].isna() | (annot[k].astype(str).str.strip()==\"\")\n",
    "            annot.loc[fill_mask, k] = annot.loc[fill_mask, k + \"_nearest\"]\n",
    "            if k + \"_nearest\" in annot.columns:\n",
    "                annot.drop(columns=[k + \"_nearest\"], inplace=True)\n",
    "\n",
    "    # ===== 把 t-test 结果合并进来 =====\n",
    "    sig_for_merge = sig.copy()\n",
    "    sig_for_merge[\"SNP\"] = sig_for_merge[\"SNP\"].astype(str)\n",
    "\n",
    "    final = annot.merge(sig_for_merge, on=\"SNP\", how=\"left\")\n",
    "\n",
    "    # 按 p_value 排个序，方便后面看\n",
    "    if \"p_value\" in final.columns:\n",
    "        final = final.sort_values(\"p_value\", na_position=\"last\")\n",
    "\n",
    "    # 输出\n",
    "    out_path = SNP_DIR / f\"{disease}_C_overlap_p0.1_annotated.csv\"\n",
    "    final.to_csv(out_path, index=False)\n",
    "    print(f\"[DONE] {disease}: 已输出 {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d93898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 从 ALZ_GENE.csv 读入阿尔兹海默相关基因数: 17508\n",
      "\n",
      "[INFO] 处理 ADHD: ADHD_C_overlap_p0.1_annotated.csv\n",
      "  总行数: 48\n",
      "  含 ALZ 相关基因的行数: 25\n",
      "  保留行数(去掉这些基因后): 23\n",
      "  已保存: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/ADHD_C_overlap_p0.1_annotated_noALZgene.csv\n",
      "\n",
      "[INFO] 处理 ASD: ASD_C_overlap_p0.1_annotated.csv\n",
      "  总行数: 42\n",
      "  含 ALZ 相关基因的行数: 19\n",
      "  保留行数(去掉这些基因后): 23\n",
      "  已保存: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/ASD_C_overlap_p0.1_annotated_noALZgene.csv\n",
      "\n",
      "[INFO] 处理 EPILE: EPILE_C_overlap_p0.1_annotated.csv\n",
      "  总行数: 13\n",
      "  含 ALZ 相关基因的行数: 5\n",
      "  保留行数(去掉这些基因后): 8\n",
      "  已保存: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/EPILE_C_overlap_p0.1_annotated_noALZgene.csv\n",
      "\n",
      "[INFO] 处理 MEMORY: MEMORY_C_overlap_p0.1_annotated.csv\n",
      "  总行数: 32\n",
      "  含 ALZ 相关基因的行数: 14\n",
      "  保留行数(去掉这些基因后): 18\n",
      "  已保存: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/MEMORY_C_overlap_p0.1_annotated_noALZgene.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== 路径设置 =====\n",
    "BASE = Path(\"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data\")\n",
    "SNP_DIR = BASE / \"Related_SNP\"\n",
    "\n",
    "# 四个 annotated 文件（你已经生成好的）\n",
    "ANNOT_FILES = {\n",
    "    \"ADHD\":   SNP_DIR / \"ADHD_C_overlap_p0.1_annotated.csv\",\n",
    "    \"ASD\":    SNP_DIR / \"ASD_C_overlap_p0.1_annotated.csv\",\n",
    "    \"EPILE\":  SNP_DIR / \"EPILE_C_overlap_p0.1_annotated.csv\",\n",
    "    \"MEMORY\": SNP_DIR / \"MEMORY_C_overlap_p0.1_annotated.csv\",\n",
    "}\n",
    "\n",
    "# ALZ 相关基因文件（从 GeneCards 下的）\n",
    "ALZ_GENE_PATH = SNP_DIR / \"ALZ_GENE.csv\"\n",
    "\n",
    "# ===== 1. 读入 ALZ 相关基因列表 =====\n",
    "alz_genes_df = pd.read_csv(ALZ_GENE_PATH)\n",
    "if \"Gene Symbol\" not in alz_genes_df.columns:\n",
    "    raise ValueError(f\"{ALZ_GENE_PATH} 中找不到 'Gene Symbol' 列，实际列为: {alz_genes_df.columns.tolist()}\")\n",
    "\n",
    "alz_gene_set = (\n",
    "    alz_genes_df[\"Gene Symbol\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace(\"\", pd.NA)\n",
    "    .dropna()\n",
    "    .unique()\n",
    ")\n",
    "alz_gene_set = set(alz_gene_set)\n",
    "\n",
    "print(f\"[INFO] 从 ALZ_GENE.csv 读入阿尔兹海默相关基因数: {len(alz_gene_set)}\")\n",
    "\n",
    "# 小工具：判断一行 Gene Name 是否包含 ALZ 相关基因\n",
    "def has_alz_gene(gene_name: str) -> bool:\n",
    "    if pd.isna(gene_name):\n",
    "        return False\n",
    "    s = str(gene_name).strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    # Gene Name 里有时是 \"GENE1;GENE2\" 这种形式\n",
    "    parts = []\n",
    "    for token in s.replace(\",\", \";\").split(\";\"):\n",
    "        token = token.strip()\n",
    "        if token:\n",
    "            parts.append(token)\n",
    "    for g in parts:\n",
    "        if g in alz_gene_set:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ===== 2. 对四个疾病文件分别过滤 =====\n",
    "for disease, in_path in ANNOT_FILES.items():\n",
    "    print(f\"\\n[INFO] 处理 {disease}: {in_path.name}\")\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    if \"Gene Name\" not in df.columns:\n",
    "        raise ValueError(f\"{in_path.name} 中找不到 'Gene Name' 列，实际列为: {df.columns.tolist()}\")\n",
    "\n",
    "    # 计算每行是否含 ALZ 相关基因\n",
    "    mask_alz = df[\"Gene Name\"].apply(has_alz_gene)\n",
    "\n",
    "    n_total = len(df)\n",
    "    n_alz = mask_alz.sum()\n",
    "    n_keep = n_total - n_alz\n",
    "\n",
    "    print(f\"  总行数: {n_total}\")\n",
    "    print(f\"  含 ALZ 相关基因的行数: {n_alz}\")\n",
    "    print(f\"  保留行数(去掉这些基因后): {n_keep}\")\n",
    "\n",
    "    df_cleaned = df[~mask_alz].copy()\n",
    "\n",
    "    # 输出：在原文件名基础上加后缀 _noALZgene\n",
    "    out_path = SNP_DIR / f\"{in_path.stem}_noALZgene.csv\"\n",
    "    df_cleaned.to_csv(out_path, index=False)\n",
    "    print(f\"  已保存: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfde0150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] 处理 ADHD: ADHD_C_overlap_p0.1_annotated_noALZgene.csv\n",
      "  ADHD 相关基因数: 3051\n",
      "  原始行数: 23\n",
      "  含 ADHD 相关基因的行数: 0\n",
      "  保留行数（去掉这些后）: 23\n",
      "  已保存: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/ADHD_C_overlap_p0.1_annotated_noALZgene_noADHDgene.csv\n",
      "\n",
      "[INFO] 处理 ASD: ASD_C_overlap_p0.1_annotated_noALZgene.csv\n",
      "  ASD 相关基因数: 15128\n",
      "  原始行数: 23\n",
      "  含 ASD 相关基因的行数: 6\n",
      "  保留行数（去掉这些后）: 17\n",
      "  已保存: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/ASD_C_overlap_p0.1_annotated_noALZgene_noASDgene.csv\n",
      "\n",
      "[INFO] 处理 EPILE: EPILE_C_overlap_p0.1_annotated_noALZgene.csv\n",
      "  EPILE 相关基因数: 10045\n",
      "  原始行数: 8\n",
      "  含 EPILE 相关基因的行数: 3\n",
      "  保留行数（去掉这些后）: 5\n",
      "  已保存: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/EPILE_C_overlap_p0.1_annotated_noALZgene_noEPILEgene.csv\n",
      "\n",
      "[INFO] 处理 MEMORY: MEMORY_C_overlap_p0.1_annotated_noALZgene.csv\n",
      "  MEMORY 相关基因数: 16691\n",
      "  原始行数: 18\n",
      "  含 MEMORY 相关基因的行数: 3\n",
      "  保留行数（去掉这些后）: 15\n",
      "  已保存: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/MEMORY_C_overlap_p0.1_annotated_noALZgene_noMEMORYgene.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path(\"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data\")\n",
    "SNP_DIR = BASE / \"Related_SNP\"\n",
    "\n",
    "# 1) 已经去掉 ALZ 基因的四个 SNP 文件\n",
    "ANNOT_NO_ALZ = {\n",
    "    \"ADHD\":   SNP_DIR / \"ADHD_C_overlap_p0.1_annotated_noALZgene.csv\",\n",
    "    \"ASD\":    SNP_DIR / \"ASD_C_overlap_p0.1_annotated_noALZgene.csv\",\n",
    "    \"EPILE\":  SNP_DIR / \"EPILE_C_overlap_p0.1_annotated_noALZgene.csv\",\n",
    "    \"MEMORY\": SNP_DIR / \"MEMORY_C_overlap_p0.1_annotated_noALZgene.csv\",\n",
    "}\n",
    "\n",
    "# 2) 四个疾病自己的 GeneCards 基因列表\n",
    "DISEASE_GENES = {\n",
    "    \"ADHD\":   SNP_DIR / \"ADHD_GENE.csv\",\n",
    "    \"ASD\":    SNP_DIR / \"ASD_GENE.csv\",\n",
    "    \"EPILE\":  SNP_DIR / \"EPILE_GENE.csv\",\n",
    "    \"MEMORY\": SNP_DIR / \"MP_GENE.csv\",   # memory performance 对应的 gene 列表\n",
    "}\n",
    "\n",
    "def load_gene_set(path: Path) -> set:\n",
    "    \"\"\"从 GeneCards 导出的 *_GENE.csv 里读 Gene Symbol 列做成集合。\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    if \"Gene Symbol\" not in df.columns:\n",
    "        raise ValueError(f\"{path.name} 中找不到 'Gene Symbol' 列，实际列为: {df.columns.tolist()}\")\n",
    "    genes = (\n",
    "        df[\"Gene Symbol\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace(\"\", pd.NA)\n",
    "        .dropna()\n",
    "        .unique()\n",
    "    )\n",
    "    return set(genes)\n",
    "\n",
    "def gene_name_has_any(gene_name: str, gene_set: set) -> bool:\n",
    "    \"\"\"判断一行 Gene Name 是否包含 gene_set 中的任意基因（支持 'A;B;C' 这种形式）\"\"\"\n",
    "    if pd.isna(gene_name):\n",
    "        return False\n",
    "    s = str(gene_name).strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    parts = []\n",
    "    # 兼容 ; 和 , 分隔\n",
    "    for token in s.replace(\",\", \";\").split(\";\"):\n",
    "        token = token.strip()\n",
    "        if token:\n",
    "            parts.append(token)\n",
    "    for g in parts:\n",
    "        if g in gene_set:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for disease, annot_path in ANNOT_NO_ALZ.items():\n",
    "    print(f\"\\n[INFO] 处理 {disease}: {annot_path.name}\")\n",
    "\n",
    "    if disease not in DISEASE_GENES:\n",
    "        print(f\"[WARN] {disease} 没有对应的基因文件映射，跳过。\")\n",
    "        continue\n",
    "\n",
    "    gene_path = DISEASE_GENES[disease]\n",
    "    if not annot_path.exists():\n",
    "        print(f\"[WARN] {annot_path} 不存在，跳过。\")\n",
    "        continue\n",
    "    if not gene_path.exists():\n",
    "        print(f\"[WARN] {gene_path} 不存在，跳过。\")\n",
    "        continue\n",
    "\n",
    "    # 读疾病基因集合\n",
    "    disease_genes = load_gene_set(gene_path)\n",
    "    print(f\"  {disease} 相关基因数: {len(disease_genes)}\")\n",
    "\n",
    "    # 读已经去掉 ALZ 基因的 SNP 注释文件\n",
    "    df = pd.read_csv(annot_path)\n",
    "    if \"Gene Name\" not in df.columns:\n",
    "        raise ValueError(f\"{annot_path.name} 中找不到 'Gene Name' 列，实际列为: {df.columns.tolist()}\")\n",
    "\n",
    "    # 标记哪些行含有该疾病相关基因\n",
    "    mask_has_self = df[\"Gene Name\"].apply(lambda x: gene_name_has_any(x, disease_genes))\n",
    "\n",
    "    n_total = len(df)\n",
    "    n_self = mask_has_self.sum()\n",
    "    n_keep = n_total - n_self\n",
    "\n",
    "    print(f\"  原始行数: {n_total}\")\n",
    "    print(f\"  含 {disease} 相关基因的行数: {n_self}\")\n",
    "    print(f\"  保留行数（去掉这些后）: {n_keep}\")\n",
    "\n",
    "    df_cleaned = df[~mask_has_self].copy()\n",
    "\n",
    "    # 输出文件名：原文件名 + _no{DISEASE}gene\n",
    "    out_path = SNP_DIR / f\"{annot_path.stem}_no{disease}gene.csv\"\n",
    "    df_cleaned.to_csv(out_path, index=False)\n",
    "    print(f\"  已保存: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411adc73",
   "metadata": {},
   "source": [
    "Random SNP test for significance of EPILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95794e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading C, ids, and SNP markers ...\n",
      "[INFO] Loading AD groups from figure3_groups.csv ...\n",
      "[INFO] AD 病人数: 251, C 中 SNP 数: 71278\n",
      "[INFO] Output directory: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/randomTest\n",
      "[RESULT] Repeat 01/10: among 100 random SNPs, count(p < 1e-06) = 53\n",
      "[RESULT] Repeat 02/10: among 100 random SNPs, count(p < 1e-06) = 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangjiahui/anaconda3/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:586: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Repeat 03/10: among 100 random SNPs, count(p < 1e-06) = 45\n",
      "[RESULT] Repeat 04/10: among 100 random SNPs, count(p < 1e-06) = 48\n",
      "[RESULT] Repeat 05/10: among 100 random SNPs, count(p < 1e-06) = 44\n",
      "[RESULT] Repeat 06/10: among 100 random SNPs, count(p < 1e-06) = 50\n",
      "[RESULT] Repeat 07/10: among 100 random SNPs, count(p < 1e-06) = 47\n",
      "[RESULT] Repeat 08/10: among 100 random SNPs, count(p < 1e-06) = 49\n",
      "[RESULT] Repeat 09/10: among 100 random SNPs, count(p < 1e-06) = 54\n",
      "[RESULT] Repeat 10/10: among 100 random SNPs, count(p < 1e-06) = 55\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "在 Prepared_data4 的 C.npy 上随机抽 100 个 SNP，做 AD group1 vs group2 的 Welch t-test\n",
    "重复 10 次：\n",
    "  - 控制台输出每次有多少 SNP 的 p < 1e-6\n",
    "  - 保存每次抽到的 100 个 SNP 及 t_stat/p_value 等到 10 个文件\n",
    "输出目录：\n",
    "  /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/randomTest\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# ------------ 路径设置 ------------\n",
    "ROOT = Path(\"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data\")\n",
    "PREP = ROOT / \"Prepared_data4\"\n",
    "SNP_DIR = ROOT / \"Related_SNP\"\n",
    "OUT_DIR = SNP_DIR / \"randomTest\"\n",
    "\n",
    "C_PATH = PREP / \"C.npy\"\n",
    "IDS_PATH = PREP / \"ids.txt\"\n",
    "NONEXON_MARKERS_PATH = PREP / \"nonexonMarkers_after_clean.txt\"\n",
    "GROUPS_PATH = PREP / \"figure3_groups.csv\"   # Fig3 脚本生成的 AD-only 分组\n",
    "\n",
    "# ------------ 参数 ------------\n",
    "N_RANDOM_SNP = 100\n",
    "N_REPEAT = 10\n",
    "P_THRESH = 1e-6\n",
    "BASE_SEED = 12345  # 你可以改成任何整数，保证可复现\n",
    "\n",
    "# ------------ 0. 创建输出目录 ------------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------ 1. 读取 C, ids, SNP 列顺序 ------------\n",
    "print(\"[INFO] Loading C, ids, and SNP markers ...\")\n",
    "C = np.load(C_PATH)                           # shape: (n_subjects, n_C_snps)\n",
    "ids = np.loadtxt(IDS_PATH, dtype=str)        # length: n_subjects\n",
    "assert C.shape[0] == len(ids), \"C 的行数与 ids 数量不一致。\"\n",
    "\n",
    "nonexon_snps = pd.read_csv(NONEXON_MARKERS_PATH, header=None)[0].astype(str).to_numpy()\n",
    "assert C.shape[1] == len(nonexon_snps), \"C 的列数与 nonexonMarkers_after_clean.txt 长度不一致。\"\n",
    "\n",
    "snp_to_col = {snp: idx for idx, snp in enumerate(nonexon_snps)}\n",
    "\n",
    "# ------------ 2. 读取 AD 分组（figure3_groups.csv） ------------\n",
    "print(\"[INFO] Loading AD groups from figure3_groups.csv ...\")\n",
    "grp_df = pd.read_csv(GROUPS_PATH)\n",
    "\n",
    "if \"id\" not in grp_df.columns or \"cluster\" not in grp_df.columns:\n",
    "    raise ValueError(\"figure3_groups.csv 应包含 'id' 和 'cluster' 两列。\")\n",
    "\n",
    "grp_df = grp_df[[\"id\", \"cluster\"]].copy()\n",
    "grp_df[\"id\"] = grp_df[\"id\"].astype(str)\n",
    "\n",
    "uniq_clusters = sorted(pd.unique(grp_df[\"cluster\"].astype(int)))\n",
    "if len(uniq_clusters) < 2:\n",
    "    raise ValueError(\"分组文件中只有一个簇，无法做两组比较。\")\n",
    "cluster_map = {uniq_clusters[0]: 1, uniq_clusters[1]: 2}\n",
    "grp_df[\"group\"] = grp_df[\"cluster\"].astype(int).map(cluster_map)\n",
    "\n",
    "id_to_idx = {sid: i for i, sid in enumerate(ids)}\n",
    "grp_df = grp_df[grp_df[\"id\"].isin(id_to_idx.keys())].copy()\n",
    "\n",
    "ad_indices = np.array([id_to_idx[sid] for sid in grp_df[\"id\"]], dtype=int)\n",
    "groups = grp_df[\"group\"].to_numpy(int)\n",
    "assert ad_indices.shape[0] == groups.shape[0], \"AD index 与 group 长度不一致。\"\n",
    "\n",
    "C_ad = C[ad_indices, :]  # shape: (n_AD, n_C_snps)\n",
    "\n",
    "print(f\"[INFO] AD 病人数: {C_ad.shape[0]}, C 中 SNP 数: {C_ad.shape[1]}\")\n",
    "print(f\"[INFO] Output directory: {OUT_DIR}\")\n",
    "\n",
    "# ------------ 3. 随机抽样 + t-test，重复 10 次 ------------\n",
    "n_total_snps = C_ad.shape[1]\n",
    "if N_RANDOM_SNP > n_total_snps:\n",
    "    raise ValueError(f\"N_RANDOM_SNP={N_RANDOM_SNP} 大于 C 的 SNP 总数={n_total_snps}。\")\n",
    "\n",
    "for rep in range(1, N_REPEAT + 1):\n",
    "    rng = np.random.default_rng(BASE_SEED + rep)\n",
    "\n",
    "    # 随机抽 100 列（不放回）\n",
    "    sampled_cols = rng.choice(n_total_snps, size=N_RANDOM_SNP, replace=False)\n",
    "    sampled_cols = np.sort(sampled_cols)\n",
    "\n",
    "    results = []\n",
    "    for col_idx in sampled_cols:\n",
    "        snp = str(nonexon_snps[col_idx])\n",
    "        x = C_ad[:, col_idx].astype(float)\n",
    "\n",
    "        g1 = x[groups == 1]\n",
    "        g2 = x[groups == 2]\n",
    "\n",
    "        # Welch t-test + nan omit\n",
    "        if len(g1) < 2 or len(g2) < 2:\n",
    "            t_stat, p_val = np.nan, np.nan\n",
    "        else:\n",
    "            t_stat, p_val = ttest_ind(g1, g2, equal_var=False, nan_policy=\"omit\")\n",
    "\n",
    "        results.append({\n",
    "            \"SNP\": snp,\n",
    "            \"C_col_index\": int(col_idx),\n",
    "            \"n_group1\": int(np.sum(groups == 1)),\n",
    "            \"n_group2\": int(np.sum(groups == 2)),\n",
    "            \"mean_group1\": float(np.nanmean(g1)) if len(g1) > 0 else np.nan,\n",
    "            \"mean_group2\": float(np.nanmean(g2)) if len(g2) > 0 else np.nan,\n",
    "            \"t_stat\": float(t_stat) if t_stat is not None else np.nan,\n",
    "            \"p_value\": float(p_val) if p_val is not None else np.nan,\n",
    "        })\n",
    "\n",
    "    res_df = pd.DataFrame(results)\n",
    "\n",
    "    # 统计 p < 1e-6 的个数（排除 NaN）\n",
    "    sig_count = int(np.sum((res_df[\"p_value\"].to_numpy(float) < P_THRESH) & (~res_df[\"p_value\"].isna())))\n",
    "    print(f\"[RESULT] Repeat {rep:02d}/{N_REPEAT}: among {N_RANDOM_SNP} random SNPs, \"\n",
    "          f\"count(p < {P_THRESH:.0e}) = {sig_count}\")\n",
    "\n",
    "    # 保存该次的 100 SNP 结果\n",
    "    out_path = OUT_DIR / f\"random_C_100snps_ttest_rep{rep:02d}.tsv\"\n",
    "    res_df.to_csv(out_path, sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b78a8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading GTF genes...\n",
      "[INFO] Parsed genes: 57,819\n",
      "[INFO] Building gene index (±10kb)...\n",
      "[INFO] Loading BIM...\n",
      "[INFO] BIM SNPs: 716,503\n",
      "[INFO] Loading non-exonic SNP list...\n",
      "[INFO] non-exonic markers: 71,278\n",
      "[INFO] Restricting BIM to non-exonic SNPs (background SNP pool)...\n",
      "[INFO] Background SNPs in BIM ∩ non-exonic: 71,278\n",
      "[INFO] Loading EPILE t-test results...\n",
      "[OK] Saved foreground SNP table: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/EPILE_fg_snps_p1e-6.tsv (n=9)\n",
      "[INFO] Extracting foreground SNP coordinates from BIM (restricted to non-exonic pool)...\n",
      "[INFO] Mapping foreground SNPs to genes (±10kb)...\n",
      "[OK] Saved foreground SNP->gene mapping: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/EPILE_fg_snp2gene_10kb.tsv\n",
      "[OK] Saved foreground gene list: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/EPILE_fg_genes_10kb.txt (n=10)\n",
      "[INFO] Mapping background SNPs to genes to construct background gene universe (may take some time)...\n",
      "[OK] Saved background gene universe: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/C_background_genes_10kb.txt (n=23020)\n",
      "[OK] Saved summary: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/mapping_summary_10kb.txt\n",
      "[DONE] All outputs written to: /Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "epile_enrichment_prep_10kb.py\n",
    "\n",
    "Foreground: EPILE SNPs with p <= 1e-6 from EPILE_C_overlap_ttest_results.tsv\n",
    "Background: all non-exonic SNPs (from nonExonMarkers_after_clean.txt) intersected with BIM\n",
    "Mapping: SNP -> genes whose gene body overlaps SNP position within ±10kb (gene_start-10kb <= pos <= gene_end+10kb)\n",
    "\n",
    "Outputs (to out_dir):\n",
    "- EPILE_fg_snps_p1e-6.tsv\n",
    "- EPILE_fg_snp2gene_10kb.tsv\n",
    "- EPILE_fg_genes_10kb.txt\n",
    "- C_background_genes_10kb.txt\n",
    "- mapping_summary_10kb.txt\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from bisect import bisect_left, bisect_right\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# User-specified paths\n",
    "# ----------------------------\n",
    "GTF_PATH = \"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/gencode.v19.annotation.gtf\"\n",
    "BIM_PATH = \"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/ADNI_GO2_GWAS_PLINK2/ADNI_GO2_GWAS_2nd_orig_BIN.bim\"\n",
    "EPILE_TTEST_PATH = \"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP/EPILE_C_overlap_ttest_results.tsv\"\n",
    "NONEXON_PATH = \"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Prepared_data4/nonExonMarkers_after_clean.txt\"\n",
    "OUT_DIR = \"/Users/zhangjiahui/Desktop/Haohan Research/Alz GWAS data/Related_SNP\"\n",
    "\n",
    "WINDOW_BP = 10_000\n",
    "P_THRESH = 1e-6\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def die(msg: str, code: int = 1):\n",
    "    print(f\"[ERROR] {msg}\", file=sys.stderr)\n",
    "    sys.exit(code)\n",
    "\n",
    "def ensure_exists(path: str, label: str):\n",
    "    if not os.path.exists(path):\n",
    "        die(f\"{label} not found: {path}\")\n",
    "\n",
    "def normalize_chr(ch):\n",
    "    \"\"\"\n",
    "    Convert chromosome labels to a consistent string without 'chr' prefix.\n",
    "    Examples:\n",
    "      'chr1' -> '1'\n",
    "      '1'    -> '1'\n",
    "      'X'    -> 'X'\n",
    "      'chrX' -> 'X'\n",
    "      'MT'/'M'/'chrM' -> 'MT' (normalize)\n",
    "    \"\"\"\n",
    "    s = str(ch).strip()\n",
    "    s = s.replace(\"chr\", \"\")\n",
    "    if s in {\"M\", \"MT\"}:\n",
    "        return \"MT\"\n",
    "    return s\n",
    "\n",
    "def parse_gtf_genes(gtf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse GTF and keep only 'gene' features.\n",
    "    Return DataFrame: chr, start, end, gene\n",
    "    \"\"\"\n",
    "    genes = []\n",
    "    # GTF columns: seqname, source, feature, start, end, score, strand, frame, attribute\n",
    "    with open(gtf_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(parts) < 9:\n",
    "                continue\n",
    "            feature = parts[2]\n",
    "            if feature != \"gene\":\n",
    "                continue\n",
    "            chrom = normalize_chr(parts[0])\n",
    "            try:\n",
    "                start = int(parts[3])\n",
    "                end = int(parts[4])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            attr = parts[8]\n",
    "            # extract gene_name \"XXX\"\n",
    "            m = re.search(r'gene_name \"([^\"]+)\"', attr)\n",
    "            if not m:\n",
    "                continue\n",
    "            gene_name = m.group(1)\n",
    "            genes.append((chrom, start, end, gene_name))\n",
    "\n",
    "    if not genes:\n",
    "        die(\"No genes parsed from GTF. Please confirm the GTF format and content.\")\n",
    "\n",
    "    df = pd.DataFrame(genes, columns=[\"chr\", \"start\", \"end\", \"gene\"]).drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def load_bim(bim_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load PLINK BIM.\n",
    "    Columns: chr, rsid, cm, pos, a1, a2\n",
    "    \"\"\"\n",
    "    bim = pd.read_csv(\n",
    "        bim_path,\n",
    "        sep=r\"\\s+\",\n",
    "        header=None,\n",
    "        names=[\"chr\", \"rsid\", \"cm\", \"pos\", \"a1\", \"a2\"],\n",
    "        dtype={\"chr\": str, \"rsid\": str, \"cm\": float, \"pos\": int, \"a1\": str, \"a2\": str},\n",
    "    )\n",
    "    bim[\"chr\"] = bim[\"chr\"].apply(normalize_chr)\n",
    "    return bim\n",
    "\n",
    "def load_nonexon_snps(nonexon_path: str) -> set:\n",
    "    snps = set()\n",
    "    with open(nonexon_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if s:\n",
    "                snps.add(s)\n",
    "    if not snps:\n",
    "        die(\"nonExonMarkers_after_clean.txt appears empty.\")\n",
    "    return snps\n",
    "\n",
    "def infer_columns_epile(df: pd.DataFrame) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Infer rsid and pvalue columns in EPILE ttest table.\n",
    "    Accepts common variations.\n",
    "    \"\"\"\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    rsid_candidates = [\"rsid\", \"snp\", \"snps\", \"marker\", \"id\"]\n",
    "    p_candidates = [\"pvalue\", \"p_value\", \"p\", \"pval\", \"p-val\", \"p.value\"]\n",
    "\n",
    "    rs_col = None\n",
    "    p_col = None\n",
    "    for k in rsid_candidates:\n",
    "        if k in cols:\n",
    "            rs_col = cols[k]\n",
    "            break\n",
    "    for k in p_candidates:\n",
    "        if k in cols:\n",
    "            p_col = cols[k]\n",
    "            break\n",
    "\n",
    "    if rs_col is None or p_col is None:\n",
    "        die(\n",
    "            \"Cannot infer rsid/pvalue columns from EPILE_C_overlap_ttest_results.tsv. \"\n",
    "            f\"Found columns: {list(df.columns)}. \"\n",
    "            \"Please rename columns to include 'rsid' and 'pvalue' (recommended) or edit infer_columns_epile().\"\n",
    "        )\n",
    "    return rs_col, p_col\n",
    "\n",
    "def build_gene_index(genes_df: pd.DataFrame, window_bp: int):\n",
    "    \"\"\"\n",
    "    Build per-chromosome sorted gene intervals with window applied.\n",
    "    Returns dict chr -> list of (start_w, end_w, gene), plus an array of starts for binary search.\n",
    "    \"\"\"\n",
    "    idx = {}\n",
    "    for chrom, gdf in genes_df.groupby(\"chr\"):\n",
    "        gdf2 = gdf.copy()\n",
    "        gdf2[\"start_w\"] = (gdf2[\"start\"] - window_bp).clip(lower=1)\n",
    "        gdf2[\"end_w\"] = gdf2[\"end\"] + window_bp\n",
    "        gdf2 = gdf2.sort_values(\"start_w\")\n",
    "\n",
    "        intervals = list(zip(gdf2[\"start_w\"].tolist(), gdf2[\"end_w\"].tolist(), gdf2[\"gene\"].tolist()))\n",
    "        starts = [x[0] for x in intervals]\n",
    "        idx[chrom] = (intervals, starts)\n",
    "    return idx\n",
    "\n",
    "def map_snp_to_genes(chrom: str, pos: int, gene_index) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return list of genes whose windowed interval covers (chrom,pos).\n",
    "    Uses binary search on interval starts then scans forward until start > pos.\n",
    "    \"\"\"\n",
    "    if chrom not in gene_index:\n",
    "        return []\n",
    "    intervals, starts = gene_index[chrom]\n",
    "    # find rightmost interval start <= pos\n",
    "    i = bisect_right(starts, pos) - 1\n",
    "    if i < 0:\n",
    "        return []\n",
    "\n",
    "    genes = []\n",
    "    # scan backwards a bit in case overlapping intervals started earlier\n",
    "    # start from i and go backwards until interval end < pos (safe bound)\n",
    "    j = i\n",
    "    while j >= 0 and intervals[j][1] >= pos:\n",
    "        j -= 1\n",
    "    j += 1  # first interval that may overlap pos\n",
    "\n",
    "    # scan forward until start > pos\n",
    "    k = j\n",
    "    n = len(intervals)\n",
    "    while k < n and intervals[k][0] <= pos:\n",
    "        if intervals[k][1] >= pos:\n",
    "            genes.append(intervals[k][2])\n",
    "        k += 1\n",
    "\n",
    "    return genes\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    ensure_exists(GTF_PATH, \"GTF\")\n",
    "    ensure_exists(BIM_PATH, \"BIM\")\n",
    "    ensure_exists(EPILE_TTEST_PATH, \"EPILE ttest results\")\n",
    "    ensure_exists(NONEXON_PATH, \"nonExonMarkers_after_clean\")\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"[INFO] Loading GTF genes...\")\n",
    "    genes_df = parse_gtf_genes(GTF_PATH)\n",
    "    print(f\"[INFO] Parsed genes: {len(genes_df):,}\")\n",
    "\n",
    "    print(\"[INFO] Building gene index (±10kb)...\")\n",
    "    gene_index = build_gene_index(genes_df, WINDOW_BP)\n",
    "\n",
    "    print(\"[INFO] Loading BIM...\")\n",
    "    bim = load_bim(BIM_PATH)\n",
    "    print(f\"[INFO] BIM SNPs: {len(bim):,}\")\n",
    "\n",
    "    print(\"[INFO] Loading non-exonic SNP list...\")\n",
    "    nonexon_snps = load_nonexon_snps(NONEXON_PATH)\n",
    "    print(f\"[INFO] non-exonic markers: {len(nonexon_snps):,}\")\n",
    "\n",
    "    print(\"[INFO] Restricting BIM to non-exonic SNPs (background SNP pool)...\")\n",
    "    bim_bg = bim[bim[\"rsid\"].isin(nonexon_snps)].copy()\n",
    "    print(f\"[INFO] Background SNPs in BIM ∩ non-exonic: {len(bim_bg):,}\")\n",
    "\n",
    "    if bim_bg.empty:\n",
    "        die(\"No overlap between BIM rsIDs and nonExonMarkers_after_clean.txt. Please verify matching rsID formats.\")\n",
    "\n",
    "    print(\"[INFO] Loading EPILE t-test results...\")\n",
    "    epile = pd.read_csv(EPILE_TTEST_PATH, sep=\"\\t\")\n",
    "    rs_col, p_col = infer_columns_epile(epile)\n",
    "\n",
    "    # ensure numeric p-values\n",
    "    epile[p_col] = pd.to_numeric(epile[p_col], errors=\"coerce\")\n",
    "    epile = epile.dropna(subset=[p_col, rs_col]).copy()\n",
    "\n",
    "    epile_sig = epile[epile[p_col] <= P_THRESH].copy()\n",
    "    if epile_sig.empty:\n",
    "        die(f\"No SNPs found with {p_col} <= {P_THRESH}. Please check threshold or column names.\")\n",
    "    epile_sig = epile_sig.sort_values(p_col)\n",
    "\n",
    "    fg_out_path = os.path.join(OUT_DIR, \"EPILE_fg_snps_p1e-6.tsv\")\n",
    "    epile_sig.to_csv(fg_out_path, sep=\"\\t\", index=False)\n",
    "    print(f\"[OK] Saved foreground SNP table: {fg_out_path} (n={len(epile_sig)})\")\n",
    "\n",
    "    fg_snps = set(epile_sig[rs_col].astype(str).tolist())\n",
    "\n",
    "    print(\"[INFO] Extracting foreground SNP coordinates from BIM (restricted to non-exonic pool)...\")\n",
    "    fg_bim = bim_bg[bim_bg[\"rsid\"].isin(fg_snps)].copy()\n",
    "\n",
    "    missing_fg = sorted(list(fg_snps - set(fg_bim[\"rsid\"])))\n",
    "    if missing_fg:\n",
    "        print(\"[WARN] Some foreground SNPs not found in BIM ∩ non-exonic list. They will be excluded.\")\n",
    "        print(\"[WARN] Missing rsIDs (first 20):\", missing_fg[:20])\n",
    "\n",
    "    if fg_bim.empty:\n",
    "        die(\"No foreground SNPs found in BIM after restricting to non-exonic list. Check rsID matching or whether foreground SNPs are indeed non-exonic.\")\n",
    "\n",
    "    # Map foreground SNPs to genes\n",
    "    print(\"[INFO] Mapping foreground SNPs to genes (±10kb)...\")\n",
    "    fg_rows = []\n",
    "    for _, row in fg_bim.iterrows():\n",
    "        chrom = row[\"chr\"]\n",
    "        pos = int(row[\"pos\"])\n",
    "        rsid = row[\"rsid\"]\n",
    "        genes = map_snp_to_genes(chrom, pos, gene_index)\n",
    "        if not genes:\n",
    "            fg_rows.append((rsid, chrom, pos, \"\"))  # keep empty mapping\n",
    "        else:\n",
    "            for g in genes:\n",
    "                fg_rows.append((rsid, chrom, pos, g))\n",
    "\n",
    "    fg_map_df = pd.DataFrame(fg_rows, columns=[\"rsid\", \"chr\", \"pos\", \"gene\"])\n",
    "    fg_map_path = os.path.join(OUT_DIR, \"EPILE_fg_snp2gene_10kb.tsv\")\n",
    "    fg_map_df.to_csv(fg_map_path, sep=\"\\t\", index=False)\n",
    "    print(f\"[OK] Saved foreground SNP->gene mapping: {fg_map_path}\")\n",
    "\n",
    "    fg_genes = sorted(set([g for g in fg_map_df[\"gene\"].tolist() if isinstance(g, str) and g.strip() != \"\"]))\n",
    "    fg_genes_path = os.path.join(OUT_DIR, \"EPILE_fg_genes_10kb.txt\")\n",
    "    with open(fg_genes_path, \"w\") as f:\n",
    "        for g in fg_genes:\n",
    "            f.write(g + \"\\n\")\n",
    "    print(f\"[OK] Saved foreground gene list: {fg_genes_path} (n={len(fg_genes)})\")\n",
    "\n",
    "    # Map background SNPs to genes (build background gene universe)\n",
    "    # This can be large; do in a memory-safe loop.\n",
    "    print(\"[INFO] Mapping background SNPs to genes to construct background gene universe (may take some time)...\")\n",
    "    bg_genes_set = set()\n",
    "    mapped_bg_snps = 0\n",
    "    for _, row in bim_bg.iterrows():\n",
    "        chrom = row[\"chr\"]\n",
    "        pos = int(row[\"pos\"])\n",
    "        genes = map_snp_to_genes(chrom, pos, gene_index)\n",
    "        if genes:\n",
    "            bg_genes_set.update(genes)\n",
    "            mapped_bg_snps += 1\n",
    "\n",
    "    bg_genes = sorted(bg_genes_set)\n",
    "    bg_genes_path = os.path.join(OUT_DIR, \"C_background_genes_10kb.txt\")\n",
    "    with open(bg_genes_path, \"w\") as f:\n",
    "        for g in bg_genes:\n",
    "            f.write(g + \"\\n\")\n",
    "    print(f\"[OK] Saved background gene universe: {bg_genes_path} (n={len(bg_genes)})\")\n",
    "\n",
    "    # Summary\n",
    "    summary_path = os.path.join(OUT_DIR, \"mapping_summary_10kb.txt\")\n",
    "    n_fg_snps_total = len(fg_snps)\n",
    "    n_fg_snps_used = fg_bim[\"rsid\"].nunique()\n",
    "    n_fg_snps_mapped = fg_map_df[fg_map_df[\"gene\"].astype(str).str.len() > 0][\"rsid\"].nunique()\n",
    "\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        f.write(\"=== Epilepsy enrichment prep summary (±10kb mapping) ===\\n\")\n",
    "        f.write(f\"GTF: {GTF_PATH}\\n\")\n",
    "        f.write(f\"BIM: {BIM_PATH}\\n\")\n",
    "        f.write(f\"non-exonic SNP list: {NONEXON_PATH}\\n\")\n",
    "        f.write(f\"EPILE t-test table: {EPILE_TTEST_PATH}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"P-threshold (foreground): {P_THRESH}\\n\")\n",
    "        f.write(f\"Foreground SNPs in t-test table (p<=thr): {len(epile_sig)}\\n\")\n",
    "        f.write(f\"Foreground SNPs requested (unique): {n_fg_snps_total}\\n\")\n",
    "        f.write(f\"Foreground SNPs found in BIM ∩ non-exonic: {n_fg_snps_used}\\n\")\n",
    "        f.write(f\"Foreground SNPs mapped to >=1 gene: {n_fg_snps_mapped}\\n\")\n",
    "        f.write(f\"Foreground genes (unique): {len(fg_genes)}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"Background SNPs (BIM ∩ non-exonic): {len(bim_bg)}\\n\")\n",
    "        f.write(f\"Background SNPs mapped to >=1 gene: {mapped_bg_snps}\\n\")\n",
    "        f.write(f\"Background genes (unique): {len(bg_genes)}\\n\")\n",
    "\n",
    "    print(f\"[OK] Saved summary: {summary_path}\")\n",
    "    print(\"[DONE] All outputs written to:\", OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
